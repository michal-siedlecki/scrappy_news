{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import bags of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests, re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "with open('stopwords/polish.stopwords.txt') as f:\n",
    "    polish_sw = [word.replace('\\n','') for word in f.readlines()]\n",
    "    \n",
    "\n",
    "class Feed:\n",
    "    def __init__(self, name, url):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "    \n",
    "    def makeSoup(self):\n",
    "        page = requests.get(self.url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        return soup\n",
    "    \n",
    "    \n",
    "    def getHeadersSport(self):\n",
    "        \n",
    "        soup = self.makeSoup()\n",
    "        \n",
    "        if self.name == 'Onet':\n",
    "            divs = soup.find_all(class_=\"sectionLine sectionLineMax\")\n",
    "            articles_sets = [div.find_all('a') for div in divs]\n",
    "            onet_dic = {}\n",
    "            for articles in articles_sets:\n",
    "                for article in articles:                    \n",
    "                    onet_dic[article.find(class_=\"title\").text] = article.attrs.get('href')\n",
    "            return onet_dic  \n",
    "        \n",
    "        if self.name == 'Onet Sport':\n",
    "            news = soup.find_all(class_='boxTitle')\n",
    "            onetsport_dic = {'name' : self.name}\n",
    "            for i in news:\n",
    "                onetsport_dic[i.text.strip()] = i.attrs.get('href')\n",
    "            return onetsport_dic\n",
    "    \n",
    "    \n",
    "\n",
    "    def getHeaders(self):\n",
    "        \n",
    "        soup = self.makeSoup()\n",
    "        \n",
    "        if self.name == 'Onet':\n",
    "            divs = soup.find_all(class_=\"boxContent\")\n",
    "            articles_sets = [div.find_all('a') for div in divs]\n",
    "            onet_dic = {'name' : self.name}     \n",
    "            for articles in articles_sets:\n",
    "                for article in articles:                    \n",
    "                    onet_dic[article.find(class_=\"title\").text] = article.attrs.get('href')\n",
    "            return onet_dic   \n",
    "        \n",
    "        \n",
    "        if self.name == 'Wirtualna Polska':\n",
    "            divs = soup.find_all(class_=\"sc-1fu2hk8-0 jIlknD\")\n",
    "            articles_sets = [div.find_all('a') for div in divs]\n",
    "            wp_dic = {'name' : self.name}         \n",
    "            for articles in articles_sets:\n",
    "                for article in articles:                \n",
    "                    if len(wp_dic) <= news_num+1:\n",
    "                        try:\n",
    "                            wp_dic[article.find_all('div')[1].text] = article.attrs.get('href')\n",
    "                        except IndexError:\n",
    "                            wp_dic[article.find('div').text] = article.attrs.get('href')\n",
    "                    else:\n",
    "                        return wp_dic\n",
    "                    \n",
    "        \n",
    "        if self.name == 'W Polityce':\n",
    "            divs = soup.find_all(class_=\"nu-lead-articles\") + soup.find_all(class_=\"nu-tile-container nu-main-col--5\")\n",
    "            articles_sets = [div.find_all('a') for div in divs]\n",
    "            wpolityce_dic = {'name' : self.name}\n",
    "            for articles in articles_sets:\n",
    "                for article in articles:\n",
    "                    if len(wpolityce_dic) <= news_num+1:\n",
    "                        wpolityce_dic[article.find(class_=\"short-title\").text] = str(self.url[:-1])+str(article.attrs.get('href'))\n",
    "                    else:\n",
    "                        return wpolityce_dic\n",
    "                    \n",
    "\n",
    "\n",
    "        \n",
    "onet = Feed('Onet', 'https://www.onet.pl/')\n",
    "onetsport = Feed('Onet Sport', 'https://sport.onet.pl/')\n",
    "wp = Feed('Wirtualna Polska', 'https://www.wp.pl/')\n",
    "wpolityce = Feed('W Polityce', 'https://wpolityce.pl/')\n",
    "sportpl = Feed('Sport', 'http://www.sport.pl/sport/0,168452.html?str=0_24519889')\n",
    "\n",
    "\n",
    "feed_list = [onet, wp, wpolityce, sportpl]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def writeOnetSport():\n",
    "    \n",
    "    with open('onet_sport.txt', 'a+') as f:\n",
    "        existing_titles = f.readlines()\n",
    "        for title, link in onetsport.getHeadersSport().items():\n",
    "            if title not in existing_titles:\n",
    "                f.write(title+'\\n')\n",
    "\n",
    "def writePolitics():\n",
    "    a = 0\n",
    "    with open('onet_polityka.txt', 'a+') as f:\n",
    "        existing_titles = f.readlines()\n",
    "        for title, link in onet.getHeaders().items():\n",
    "            if a<=12:\n",
    "                title = title.strip()\n",
    "                if title not in existing_titles:\n",
    "                    if title != 'name':\n",
    "                        f.write(title+'\\n')\n",
    "\n",
    "writeOnetSport()                      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract key words from article headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starcie 0.681\n",
      "zapisał 0.577\n",
      "lewandowski 0.577\n",
      "historii 0.577\n",
      "wiadomości 0.562\n",
      "dobre 0.562\n",
      "turcji 0.519\n",
      "grozy 0.519\n",
      "chwile 0.519\n",
      "rewolucji 0.511\n",
      "ratunek 0.511\n",
      "niechciani 0.511\n",
      "świata 0.5\n",
      "zwycięstwo 0.5\n",
      "wicemistrz 0.5\n",
      "stocha 0.5\n",
      "skomentował 0.5\n",
      "małysz 0.5\n",
      "kubicy 0.5\n",
      "kadrze 0.5\n",
      "jerzego 0.5\n",
      "debiuty 0.5\n",
      "brzęczka 0.5\n",
      "bolesne 0.5\n",
      "wielki 0.5\n",
      "transferowy 0.5\n",
      "problemem 0.5\n",
      "oficjalnie 0.5\n",
      "ofensywa 0.5\n",
      "niemieckie 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "with open('stopwords/polish.stopwords.txt') as f:\n",
    "    polish_sw = [word.replace('\\n','') for word in f.readlines()]\n",
    "    \n",
    "with open('onet_sport.txt') as f:\n",
    "    docs = f.read().split('\\n')[:-1]\n",
    "    \n",
    "\n",
    "def pre_process(text):\n",
    "    \n",
    "    text=text.lower()\n",
    "    text=re.sub(\"<!--?.*?-->\",\"\",text)\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    " \n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "docs = list(map(lambda x:pre_process(x), docs))\n",
    "\n",
    "\n",
    "cv=CountVectorizer(max_df=0.85,stop_words=polish_sw)\n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "# you only needs to do this once, this is a mapping of index to \n",
    "feature_names=cv.get_feature_names()\n",
    " \n",
    "# get the document that we want to extract keywords from\n",
    "# doc=corpus[1]\n",
    " \n",
    "#generate tf-idf for the given document\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform(corpus))\n",
    " \n",
    "#sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    " \n",
    "#extract only the top n; n here is 10\n",
    "keywords=extract_topn_from_vector(feature_names,sorted_items,30)\n",
    "\n",
    "for k,v in keywords.items():\n",
    "    print(k,v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
